{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis Module in D-Fence Paper Code\n",
    "\n",
    "This Jupyter Notebook implements the text classification module described in the D-Fence paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The text classification module in the D-Fence paper follows these key steps:\n",
    "\n",
    "1. **Text Extraction**: Extract text from both text/plain and text/html sections.\n",
    "2. **Language Detection**: Detect the language (only English is processed).\n",
    "3. **Text Preprocessing**: Remove non-text elements like URLs, email addresses, and special characters.\n",
    "4. **BERT Tokenization and Embedding**:\n",
    "    - Tokenize the cleaned text using BERT’s tokenizer.\n",
    "    - Generate embeddings using a BERT-base model (12 layers, 768 hidden units).\n",
    "    - Aggregate token embeddings by averaging them.\n",
    "5. **Classification**: Use RandomForest or XGBoost as the final classification model.\n",
    "\n",
    "Below is the Python code implementing this pipeline. It follows the exact methodology described in the paper, using transformers for BERT embeddings and sklearn for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the missing numpy package\n",
    "%pip install numpy\n",
    "%pip install torch\n",
    "%pip install transformers\n",
    "%pip install scikit-learn\n",
    "%pip install pandas\n",
    "%pip install matplotlib\n",
    "%pip install numpy==2.1.0\n",
    "%pip install shap\n",
    "%pip install xgboost\n",
    "%pip install lime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the BERT model and tokenizer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### when you do it for the first time for SpamAssassin ###\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"Data/SpamAssasin.csv\" # Path to the dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Select only 'body' (email content) and 'label' (spam or not)\n",
    "df = df[['body', 'label']].dropna()\n",
    "print(f\"Dataset loaded: {len(df)} emails\\n\")\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "print(\"Loading BERT model...\")\n",
    "MODEL_NAME = \"bert-base-uncased\" # doesn't care about capital letters\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert_model = BertModel.from_pretrained(MODEL_NAME)\n",
    "print(\"BERT model loaded successfully!\\n\")\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    text = re.sub(r\"\\S+@\\S+\\.\\S+\", \"\", text)  # Remove email addresses\n",
    "    text = re.sub(r\"[^A-Za-z0-9\\s]\", \"\", text)  # Remove special characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize spaces\n",
    "    return text.lower()\n",
    "\n",
    "# Function to extract BERT embeddings\n",
    "def extract_bert_embeddings(text, index, total):\n",
    "    print(f\"Processing email {index+1}/{total}...\", end=\"\\r\")\n",
    "    inputs = tokenizer(text, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "\n",
    "    # Get last hidden state (512 tokens × 768 features)\n",
    "    hidden_states = outputs.last_hidden_state.squeeze(0)\n",
    "\n",
    "    # Average across tokens to get a single 768-dimensional vector\n",
    "    feature_vector = torch.mean(hidden_states, dim=0).numpy()\n",
    "    \n",
    "    return feature_vector\n",
    "\n",
    "# Apply text preprocessing\n",
    "print(\"Preprocessing email content...\")\n",
    "print(f\"Selected {len(df)} emails from the dataset.\")\n",
    "df['body'] = df['body'].apply(preprocess_text)\n",
    "print(\"Text preprocessing complete!\\n\")\n",
    "\n",
    "# Extract embeddings for all emails\n",
    "print(\"Extracting BERT embeddings for each email...\")\n",
    "features = np.array([extract_bert_embeddings(email, i, len(df)) for i, email in enumerate(df['body'])])\n",
    "print(\"\\nBERT embeddings extraction complete!\\n\")\n",
    "\n",
    "# Save features to a file\n",
    "np.save(\"bert_features_1.npy\", features)\n",
    "print(\"Features saved to bert_features.npy\\n\")\n",
    "\n",
    "labels = df['label'].values  # Target labels (0 = benign, 1 = phishing)\n",
    "\n",
    "# Split into train and test sets\n",
    "print(\"Splitting dataset into training and testing sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "print(f\"Training set: {len(X_train)} emails, Test set: {len(X_test)} emails\\n\")\n",
    "\n",
    "# Train an XGBoost classifier\n",
    "print(\"Training XGBoost classifier...\")\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "print(\"Training complete!\\n\")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating XGBoost model performance...\\n\")\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "print(\"\\n✅ Phishing detection pipeline using XGBoost complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### when you already have bert_features.npy for SpamAssassin ###\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Load the saved BERT features\n",
    "print(\"Loading BERT features from file...\")\n",
    "features = np.load(\"bert_features.npy\")\n",
    "\n",
    "# Ensure labels are available (should be loaded from the dataset)\n",
    "file_path = \"Data/SpamAssasin.csv\"  # Adjust as needed\n",
    "df = pd.read_csv(file_path)\n",
    "df = df[['body', 'label']].dropna()\n",
    "labels = df['label'].values  # Load labels from the original dataset\n",
    "\n",
    "# Split into train and test sets\n",
    "print(\"Splitting dataset into training and testing sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "print(f\"Training set: {len(X_train)} emails, Test set: {len(X_test)} emails\\n\")\n",
    "\n",
    "# Train an XGBoost classifier\n",
    "print(\"Training XGBoost classifier...\")\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False, eval_metric=\"logloss\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "print(\"Training complete!\\n\")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating XGBoost model performance...\\n\")\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Print the classification report\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "# AUPRC calculation\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred_xgb)\n",
    "auprc = auc(recall, precision)\n",
    "print(f\"AUPRC (Area Under Precision-Recall Curve): {auprc:.4f}\")\n",
    "\n",
    "# recall at fixed FPR\n",
    "desired_fpr = 0.01 # Define the desired FPR threshold (example: FPR = 0.001, meaning 0.1%)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_xgb) # Compute False Positive Rate (FPR) and True Positive Rate (TPR) from ROC curve\n",
    "closest_index = (np.abs(fpr - desired_fpr)).argmin() # Find the closest FPR value in the computed ROC curve\n",
    "selected_tpr = tpr[closest_index] # Select the TPR value corresponding to the closest FPR value\n",
    "print(f\"TPR (Recall) at FPR={desired_fpr}: {selected_tpr:.6f}\") # Print the TPR (Recall) at the fixed FPR\n",
    "\n",
    "\n",
    "print(\"\\n✅ Phishing detection pipeline using XGBoost complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a classifier (Random Forest)\n",
    "print(\"Training Random Forest classifier...\")\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Training complete!\\n\")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating model performance...\\n\")\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# AUPRC calculation\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
    "auprc = auc(recall, precision)\n",
    "print(f\"AUPRC (Area Under Precision-Recall Curve): {auprc:.4f}\")\n",
    "\n",
    "# recall at fixed FPR\n",
    "desired_fpr = 0.01 # Define the desired FPR threshold (example: FPR = 0.001, meaning 0.1%)\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred) # Compute False Positive Rate (FPR) and True Positive Rate (TPR) from ROC curve\n",
    "closest_index = (np.abs(fpr - desired_fpr)).argmin() # Find the closest FPR value in the computed ROC curve\n",
    "selected_tpr = tpr[closest_index] # Select the TPR value corresponding to the closest FPR value\n",
    "print(f\"TPR (Recall) at FPR={desired_fpr}: {selected_tpr:.6f}\") # Print the TPR (Recall) at the fixed FPR\n",
    "\n",
    "\n",
    "print(\"\\n✅ Phishing detection pipeline complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this kind of pipeline we cannot use SHAP because the input to XGBoost is the precomputed BERT embeddings, not raw words or tokens. We need to modify our pipeline so that SHAP can work directly on the raw text before it is converted into BERT embeddings.\n",
    "\n",
    "Instead of training XGBoost on BERT embeddings, you will fine-tune a BERT classifier instead of using precomputed embeddings and use SHAP’s DeepExplainer or GradientExplainer on the BERT model to explain feature importance at the token level."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
