{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To systematically test different hyperparameter combinations, we will implement grid search using the specified values. The different hyperparameters are based on the recommended values from section fine tuning bert. We use resources from Grid5000 to get a GPU.\n",
    "\n",
    "1. Iterate through different combinations of:\n",
    "    - Batch size (16, 32)\n",
    "    - Learning rate (2e-5, 3e-5, 5e-5)\n",
    "    - Epochs (2, 3, 4)\n",
    "2.  Use Adam optimizer with learning rate warmup over 10,000 steps.\n",
    "3.  Apply dropout rate of 0.1 in the BERT model.\n",
    "4.  Use cross-entropy loss function.\n",
    "5.  Save only the best model based on accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total Training Time Estimate\n",
    "\n",
    "Assuming:\n",
    "- 30,000 samples\n",
    "- Single GPU (e.g., A100 40GB)\n",
    "- BERT-Base Uncased\n",
    "- Dataset fully fits in memory\n",
    "- No major bottlenecks in data loading\n",
    "\n",
    "![Alt Text](runtime.png)\n",
    "\n",
    "Total of 288 minutes = 4.8 h ~ 5h\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Loading datasets...\n",
      "label\n",
      "0    67\n",
      "1    33\n",
      "Name: count, dtype: int64\n",
      "Datasets loaded!\n",
      "\n",
      "Preprocessing email content...\n",
      "Text preprocessing complete!\n",
      "\n",
      "Training set: 80 emails, Test set: 20 emails\n",
      "\n",
      "\n",
      "Training with batch_size=16, lr=2e-05, epochs=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/charlinemeurant/Documents/UCL/M2/M2Q2/Code/memoire/.venv/lib/python3.13/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 - Avg Loss: 0.6593\n",
      "Epoch 2/2 - Avg Loss: 0.6467\n",
      "AUPRC: 0.4382\n",
      "\n",
      "Final Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.92      0.71        12\n",
      "           1       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.55        20\n",
      "   macro avg       0.29      0.46      0.35        20\n",
      "weighted avg       0.35      0.55      0.43        20\n",
      "\n",
      "\n",
      "Training with batch_size=16, lr=2e-05, epochs=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/charlinemeurant/Documents/UCL/M2/M2Q2/Code/memoire/.venv/lib/python3.13/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, classification_report\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "# Load dataset\n",
    "file_path1 = \"Data/SpamAssasin.csv\"\n",
    "file_path2 = \"Data/Enron.csv\"\n",
    "file_path3 = \"Data/Nazario.csv\"\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "df1 = pd.read_csv(file_path1)\n",
    "df2 = pd.read_csv(file_path2)\n",
    "df3 = pd.read_csv(file_path3)\n",
    "\n",
    "# Combine datasets\n",
    "df_combined = pd.concat([df1, df2, df3], ignore_index=True)\n",
    "df = df_combined[['body', 'label']].dropna()\n",
    "df = df.sample(n=100, random_state=42)  # Select only 100 emails for faster training\n",
    "print(df[\"label\"].value_counts())\n",
    "print(\"Datasets loaded!\\n\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"\\d+\", \"\", text)  # Remove numbers\n",
    "    text = re.sub(r\"\\S+@\\S+\\.\\S+\", \"\", text)  # Remove email addresses\n",
    "    text = re.sub(r\"[^A-Za-z0-9\\s]\", \"\", text)  # Remove special characters\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize spaces\n",
    "    return text.lower()\n",
    "\n",
    "# Apply text preprocessing\n",
    "print(\"Preprocessing email content...\")\n",
    "df['body'] = df['body'].apply(preprocess_text)\n",
    "print(\"Text preprocessing complete!\\n\")\n",
    "\n",
    "# Load BERT tokenizer\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['body'], df['label'], test_size=0.2, random_state=42)\n",
    "print(f\"Training set: {len(X_train)} emails, Test set: {len(X_test)} emails\\n\")\n",
    "\n",
    "# Define PyTorch Dataset class\n",
    "class SpamDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.texts = texts.tolist()\n",
    "        self.labels = labels.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Define hyperparameter search space\n",
    "batch_sizes = [16, 32]\n",
    "learning_rates = [2e-5, 3e-5, 5e-5]\n",
    "epochs_list = [2, 3, 4]\n",
    "\n",
    "best_auprc = 0.0\n",
    "best_model = None\n",
    "best_hyperparams = {}\n",
    "\n",
    "# Iterate through hyperparameter combinations\n",
    "for batch_size in batch_sizes:\n",
    "    for lr in learning_rates:\n",
    "        for epochs in epochs_list:\n",
    "            print(f\"\\nTraining with batch_size={batch_size}, lr={lr}, epochs={epochs}\")\n",
    "\n",
    "            # Create DataLoaders\n",
    "            train_dataset = SpamDataset(X_train, y_train, tokenizer)\n",
    "            test_dataset = SpamDataset(X_test, y_test, tokenizer)\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            # Initialize BERT model with dropout 0.1\n",
    "            bert_model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "            bert_model.to(device)\n",
    "\n",
    "            # Define optimizer with learning rate warmup over 10,000 steps\n",
    "            optimizer = AdamW(bert_model.parameters(), lr=lr)\n",
    "            total_steps = len(train_loader) * epochs\n",
    "            scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=10000, num_training_steps=total_steps)\n",
    "\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "            # Training loop\n",
    "            bert_model.train()\n",
    "            for epoch in range(epochs):\n",
    "                total_loss = 0\n",
    "                for batch_idx, batch in enumerate(train_loader):\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    input_ids = batch[\"input_ids\"].to(device)\n",
    "                    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                    labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                    outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "                    loss = criterion(outputs.logits, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "                    total_loss += loss.item()\n",
    "\n",
    "                avg_loss = total_loss / len(train_loader)\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            # Evaluate model using AUPRC\n",
    "            bert_model.eval()\n",
    "            y_preds = []\n",
    "            y_probs = []\n",
    "            y_true = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in test_loader:\n",
    "                    input_ids = batch[\"input_ids\"].to(device)\n",
    "                    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                    labels = batch[\"labels\"].cpu().numpy()\n",
    "\n",
    "                    outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "                    probs = torch.nn.functional.softmax(outputs.logits, dim=1).cpu().numpy()[:, 1]  # Get positive class probs\n",
    "                    preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n",
    "\n",
    "                    y_probs.extend(probs)  # Probabilities for AUPRC\n",
    "                    y_preds.extend(preds)\n",
    "                    y_true.extend(labels)\n",
    "\n",
    "            # Calculate AUPRC\n",
    "            auprc = average_precision_score(y_true, y_probs)\n",
    "            print(f\"AUPRC: {auprc:.4f}\")\n",
    "\n",
    "            # Print classification report\n",
    "            print(\"\\nFinal Model Performance:\")\n",
    "            print(classification_report(y_true, y_preds))\n",
    "\n",
    "            # Save the best model\n",
    "            if auprc > best_auprc:\n",
    "                best_auprc = auprc\n",
    "                best_model = bert_model\n",
    "                best_hyperparams = {\"batch_size\": batch_size, \"learning_rate\": lr, \"epochs\": epochs}\n",
    "\n",
    "# Save the best model\n",
    "MODEL_SAVE_PATH = \"best_bert_spam_classifier_auprc.pth\"\n",
    "print(f\"\\nBest model found with AUPRC {best_auprc:.4f}: {best_hyperparams}\")\n",
    "torch.save(best_model.state_dict(), MODEL_SAVE_PATH)\n",
    "print(f\"Best model saved to {MODEL_SAVE_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
