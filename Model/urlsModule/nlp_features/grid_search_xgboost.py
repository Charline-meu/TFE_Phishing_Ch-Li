import pandas as pd
import numpy as np

from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import average_precision_score

# ğŸ”„ 1. Charger le dataset des 60% balanced
df = pd.read_csv("enron_spamassassin_nazario_nigerian_best_compo/extracted_urls/combined_train_set_features_filtered.csv")

# ğŸ§¹ 2. Nettoyage
df = df[df['url'] != '-1'].copy()
df.drop_duplicates(subset='url', inplace=True)

# ğŸ¯ 3. SÃ©parer features et labels
X = df.drop(columns=['label', 'id', 'url'], errors='ignore')
y = df['label'].astype(int)

# ğŸ” 4. DÃ©finir la grille d'hyperparamÃ¨tres
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.001, 0.01, 0.1, 0.2],
    'subsample': [0.6,0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

# ğŸ§  5. DÃ©finir le modÃ¨le XGBoost de base
xgb_clf = XGBClassifier(
    eval_metric='logloss',
    random_state=42,
    n_jobs=-1
)


# ğŸ” 6. Grid Search avec validation croisÃ©e
grid_search = GridSearchCV(
    estimator=xgb_clf,
    param_grid=param_grid,
    scoring='average_precision',  # AUPRC
    cv=5,
    verbose=2,
    n_jobs=-1
)

# ğŸš€ 7. Lancer le grid search
print("\nğŸ” Grid Search en cours...")
grid_search.fit(X, y)

# âœ… 8. RÃ©sultats
print("\nâœ… Meilleurs paramÃ¨tres trouvÃ©s :")
print(grid_search.best_params_)

print(f"\nğŸ¯ Meilleur score AUPRC (moyenne CV) : {grid_search.best_score_:.4f}")

# ğŸ’¾ 9. Sauvegarder le meilleur modÃ¨le si tu veux
best_model = grid_search.best_estimator_
best_model.save_model("xgboost_best_model.json")  # facultatif
