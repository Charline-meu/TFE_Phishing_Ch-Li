import pandas as pd
from sklearn.metrics import (
    classification_report, accuracy_score, precision_score,
    recall_score, f1_score, average_precision_score
)
from sklearn.model_selection import StratifiedKFold
import shap
import xgboost as xgb
import matplotlib.pyplot as plt
import numpy as np

# ğŸ”„ 1. Charger & fusionner les jeux de donnÃ©es
df1 = pd.read_csv("enron_spamassassin_nazario_nigerian_best_compo/extracted_urls/combined_train_set_features_filtered.csv")
df2 = pd.read_csv("enron_spamassassin_nazario_nigerian_best_compo/extracted_urls/urls_train_set_20_features_filtered.csv")
df3 = pd.read_csv("enron_spamassassin_nazario_nigerian_best_compo/extracted_urls/urls_test_set_features_filtered.csv")
#df4 = pd.read_csv("urlsModule/nlp_features/nlp_features_ebu_numerique.csv")

full_df = pd.concat([df1, df2, df3], ignore_index=True)
#full_df = df4

# 3. Supprimer les doublons d'URL
full_df = full_df[full_df['url'] != '-1'].copy()
full_df.drop_duplicates(subset='url', inplace=True)

X = full_df.drop(columns=['label', 'id', 'url'], errors='ignore')
y = full_df['label'].astype(int)

# ğŸ” 2. Cross-validation
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)

# Stockage global
all_preds = []
all_probas = []
all_true = []
all_ids = []

# Pour les moyennes
accuracy_scores = []
precision_scores = []
recall_scores = []
f1_scores = []
auprc_scores = []

for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), 1):
    print(f"\nğŸ”„ Fold {fold}/10")

    X_train_fold, X_test_fold = X.iloc[train_idx], X.iloc[test_idx]
    y_train_fold, y_test_fold = y.iloc[train_idx], y.iloc[test_idx]
    ids_test_fold = full_df.iloc[test_idx]['id'].values

    model = xgb.XGBClassifier(
        n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42,
        eval_metric="logloss"
    )
    model.fit(X_train_fold, y_train_fold)

    y_pred = model.predict(X_test_fold)
    y_proba = model.predict_proba(X_test_fold)[:, 1]

    acc = accuracy_score(y_test_fold, y_pred)
    prec = precision_score(y_test_fold, y_pred)
    rec = recall_score(y_test_fold, y_pred)
    f1 = f1_score(y_test_fold, y_pred)
    auprc = average_precision_score(y_test_fold, y_proba)

    print(f"âœ… Accuracy: {acc:.4f}, Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}, AUPRC: {auprc:.4f}")

    # Stocker
    accuracy_scores.append(acc)
    precision_scores.append(prec)
    recall_scores.append(rec)
    f1_scores.append(f1)
    auprc_scores.append(auprc)

    all_preds.extend(y_pred)
    all_probas.extend(y_proba)
    all_true.extend(y_test_fold)
    all_ids.extend(ids_test_fold)

# ğŸ§¾ 3. CrÃ©er un fichier avec rÃ©sultats finaux
results_df = pd.DataFrame({
    "id": all_ids,
    "phishing_probability": all_probas,
    "predicted_label": all_preds,
    "true_label": all_true
})
results_df.to_csv("urlsModule/nlp_features/xgb_cv_results.csv", index=False)

# ğŸ“Š 4. Moyennes globales
print("\nğŸ“Š RÃ©sumÃ© global des scores (moyenne Â± std sur 10 folds):")
print(f"âœ… Accuracy       : {np.mean(accuracy_scores):.4f} Â± {np.std(accuracy_scores):.4f}")
print(f"âœ… Precision      : {np.mean(precision_scores):.4f} Â± {np.std(precision_scores):.4f}")
print(f"âœ… Recall         : {np.mean(recall_scores):.4f} Â± {np.std(recall_scores):.4f}")
print(f"âœ… F1-score       : {np.mean(f1_scores):.4f} Â± {np.std(f1_scores):.4f}")
print(f"ğŸ¯ AUPRC          : {np.mean(auprc_scores):.4f} Â± {np.std(auprc_scores):.4f}")
print("âœ… RÃ©sultats sauvegardÃ©s dans xgb_cv_results.csv")

# ğŸ” 5. SHAP sur tout le dataset (derniÃ¨re version du modÃ¨le utilisÃ©)
print("\nğŸ” SHAP analysis (last model):")
explainer = shap.Explainer(model, X)
shap_values = explainer(X)

shap.summary_plot(shap_values, X)
shap.plots.bar(shap_values.mean(0), max_display=30)